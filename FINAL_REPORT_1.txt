
================================================================================
PART 3 FINAL CHALLENGE - Phase I 
================================================================================

Step 3.1 to 3.4

================================================================================
SECTION 1: PROBLEM STATEMENT & APPROACH
================================================================================

1.1 Problem Statement
─────────────────────
Clearly state what problem you're solving:
  • Dataset: Food-101 (101 food categories, ~101k images, or subset)
  • Objective: Classification.
  • Performance Goal: 80%
  • Constraints: GPU performance

Example:
  "Build a CNN to classify food images into 101 categories with >80% accuracy
   using efficient training on GPU with early stopping."


1.2 Approach
────────────────────
Describe baseline approach (step 3.3):
  • Model architecture: CNN, 4 blocks(3 input channels->32->64->128->256 filters)
  • Training strategy: Adam optimizer, LR=0.001, CrossEntropyLoss
  • Data handling: flips, rotation, color jitter
  • Evaluation: accuracy, precision, recall, F1-score

Example:
  "We started with a custom CNN with 4 conv blocks (32→64→128→256 filters),
   trained with Adam optimizer, basic augmentation, and early stopping."


================================================================================
SECTION 2: IMPLEMENTATION DETAILS
================================================================================

2.1 Model Architecture
──────────────────────
Describe the baseline model in step 3.2:

a) Architecture Choice: Custom CNN 

   If Custom CNN:
     • Number of layers: 4
     • Filter progression: 32→64→128→256
     • Total parameters: 1464581
     • Key components: BatchNorm, Dropout, GlobalAvgPool

   If Transfer Learning:
     • Base model: [ResNet18/ResNet50/EfficientNet/etc.]
     • Frozen layers: [Which ones]
     • Fine-tuned layers: [Which ones]
     • Custom head: [Architecture]

   Example:
     "Custom CNN with 4 convolutional blocks with batch normalization.
      Progressive filter expansion: 32→64→128→256.
      Global average pooling to reduce dimensions.
      Two fully connected layers (512, 256) with dropout.
      Total: 3.2M parameters."


2.2 Data Pipeline
─────────────────
Describe your data handling:

a) Data Augmentation:
   • Techniques used: Flips, rotation, color jitter, affine
   • Why: Increase diversity, reduce overfitting
   • Effect on training: Reducing overfitting by ~5%

   Example:
     "Applied random horizontal/vertical flips, rotation (15°),
      color jitter, and affine transformations.
      Increased generalization and reduced overfitting by ~5%."

b) Data Preprocessing:
   • Image size: 224×224
   • Normalization: Normalized using ImageNet statistics (mean, std)
   • Train/test split: 75750 / 25250
   • Batch size: 32

   Example:
     "Resized all images to 224×224 (from 512×512).
      Normalized using ImageNet statistics (mean, std).
      Used 75/25 train/test split (75,750 / 25,250 samples).
      Batch size: 32 (balance between speed and memory)."


2.3 Training Configuration
──────────────────────────
Document your training setup:

a) Hyperparameters:
   • Learning rate: 0.001
   • Optimizer: Adam
   • Loss function: CrossEntropyLoss
   • Batch size: 32
   • Epochs: 15

   Example:
     "Adam optimizer with LR=0.001, no weight decay.
      CrossEntropyLoss for multi-class classification.
      Batch size: 32, max epochs: 30."

b) Regularization:
   • Dropout: [Values]
   • Weight decay: [Value]
   • Early stopping: [Patience]
   • Others: [List]

   Example:
     "Dropout: 0.5 in FC layers, 0.15 in conv blocks.
      Weight decay: 1e-4 (L2 regularization).
      Early stopping patience: 5 epochs.
      Gradient clipping: max_norm=1.0."

c) Learning Rate Schedule:
   • Type: ReduceLROnPlateau
   • Parameters: Patience=3, factor=0.5, min_lr=1e-6

   Example:
     "Used ReduceLROnPlateau: reduce LR by 0.5x when loss plateaus.
      Patience: 3 epochs, minimum LR: 1e-6."

================================================================================
